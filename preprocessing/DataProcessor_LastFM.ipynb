{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Construction - LastFM\n",
    "\n",
    "This notebook aims to model any recommender system datasets to the unified sequential format of pure text string, serving as the input of the user simulator LLM.\n",
    "\n",
    "A recommender system data usually consists of 3 critical components: user, item and behavior. The user and item are usually represented by their IDs, while the behavior is usually represented by their interactions.\n",
    "\n",
    "The goal of this notebook is to transform the original data into a unified format, with two types of JSON format:\n",
    "\n",
    "- Item Feature JSON: This JSON contains a list of all items in the dataset, each element is following the format:\n",
    "```json\n",
    "{\n",
    "    \"item_id\": \"item_id (str), a unique identifier for the item\",\n",
    "    \"item_description\": \"item_description (dict[str, str]), a dictionary of item attributes and their values. All values should be processed to be pure text string that can be understood by human. All features that contain unreadable information (e.g. image, video, audio, url, ID string ,etc.) should be converted (when possible) or removed (when not possible) to be pure text string.\",\n",
    "    \"item_features\": \"item_features (dict[str, Any]), a dictionary of item features (except for those in item_description) and their values. \"\n",
    "This JSON is stored as `<DATASET_NAME>_item_feature.jsonl`.\n",
    "\n",
    "- User Feature JSON: This JSON contains a list of all items in the dataset, each element is following the format:\n",
    "```json\n",
    "{\n",
    "    \"user_id\": \"user_id (str), a unique identifier for the user\",\n",
    "    \"user_description\": \"user_description (dict[str, str]), a dictionary of user attributes and their values. All values should be processed to be pure text string that can be understood by human. All features that contain unreadable information (e.g. image, video, audio, url, ID string ,etc.) should be converted (when possible) or removed (when not possible) to be pure text string.\",\n",
    "    \"user_features\": \"user_features (dict[str, Any]), a dictionary of user features (except for those in user_description) and their values. \"\n",
    "}\n",
    "```\n",
    "This JSON is stored as `<DATASET_NAME>_user_feature.jsonl`.\n",
    "\n",
    "- Interaction JSON: This JSON contains a list of all user behaviors in the dataset, each element is following the format:\n",
    "```json\n",
    "{\n",
    "    \"user_id\": \"user_id (str), a unique identifier for the user\",\n",
    "    \"item_id\": \"item_id (str), the ID of the item that the user has interacted with\",\n",
    "    \"timestamp\": \"timestamp (str), the timestamp of the interaction in the format of YYYY-MM-DD HH:MM:SS. When the timestamp is not available, it should be set to the random timestamp.\",\n",
    "    \"behavior_features\": \"behavior_features (dict[str, Any]), a dictionary of behavior features and their values. All values should be processed to be pure text string that can be understood by human. All features that contain unreadable information (e.g. image, video, audio, url, ID string ,etc.) should be converted (when possible) or removed (when not possible) to be pure text string.\"\n",
    "}\n",
    "```\n",
    "This JSON is stored as `<DATASET_NAME>_interaction.jsonl`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the raw data\n",
    "\n",
    "You can download the raw data from the following links:\n",
    "- [LastFM](https://files.grouplens.org/datasets/hetrec2011/hetrec2011-lastfm-2k.zip)\n",
    "\n",
    "After downloading the raw data, you can unzip the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Setting\n",
    "DATASET_NAME = \"LastFM\"\n",
    "DATASET_PATH = \"<SOURCE_PATH>\"\n",
    "OUTPUT_PATH = \"<PROJECT_PATH>/raws/\"\n",
    "MIN_INTERACTION_CNT = 5 \n",
    "MAX_INTERACTION_CNT = 20  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load the dataset\n",
    "# This step loads the dataset as a pandas dataframe and displays the shape and head of the dataframe\n",
    "# Since the structure of each dataset is different, the loading needs to be done according to the actual situation of the dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Assume the directory path where the dataset is located\n",
    "\n",
    "# Define the paths of each file\n",
    "artists_file = os.path.join(DATASET_PATH, \"artists.dat\")\n",
    "tags_file = os.path.join(DATASET_PATH, \"tags.dat\")\n",
    "user_artists_file = os.path.join(DATASET_PATH, \"user_artists.dat\")\n",
    "user_friends_file = os.path.join(DATASET_PATH, \"user_friends.dat\")\n",
    "user_taggedartists_ts_file = os.path.join(DATASET_PATH, \"user_taggedartists-timestamps.dat\")\n",
    "user_taggedartists_file = os.path.join(DATASET_PATH, \"user_taggedartists.dat\")\n",
    "\n",
    "# ---------------------------\n",
    "# Load the data (specify the correct encoding)\n",
    "# ---------------------------\n",
    "# Load the artists.dat file, keeping only the id and name columns, used for item features and subsequent artist_name lookup\n",
    "artists_df = pd.read_csv(\n",
    "    artists_file,\n",
    "    sep=\"\\t\",\n",
    "    usecols=[\"id\", \"name\"],\n",
    "    encoding=\"utf-8\"      # Set to UTF-8 according to the detection result\n",
    ")\n",
    "\n",
    "# Load the tags.dat file, used to map tagID to tagValue\n",
    "tags_df = pd.read_csv(\n",
    "    tags_file,\n",
    "    sep=\"\\t\",\n",
    "    encoding=\"ISO-8859-1\"  # Set to ISO-8859-1 according to the detection result\n",
    ")\n",
    "\n",
    "# Load the user_artists.dat file, containing userID, artistID, weight\n",
    "user_artists_df = pd.read_csv(\n",
    "    user_artists_file,\n",
    "    sep=\"\\t\",\n",
    "    encoding=\"ascii\"      # Set to ASCII according to the detection result\n",
    ")\n",
    "\n",
    "# Load the user_friends.dat file (no additional processing is done this time, but still loaded to display the data)\n",
    "user_friends_df = pd.read_csv(\n",
    "    user_friends_file,\n",
    "    sep=\"\\t\",\n",
    "    encoding=\"ascii\"\n",
    ")\n",
    "\n",
    "# Load the user_taggedartists-timestamps.dat file, containing userID, artistID, tagID, timestamp\n",
    "user_taggedartists_ts_df = pd.read_csv(\n",
    "    user_taggedartists_ts_file,\n",
    "    sep=\"\\t\",\n",
    "    encoding=\"ascii\"\n",
    ")\n",
    "\n",
    "# Load the user_taggedartists.dat file, containing userID, artistID, tagID, day, month, year\n",
    "user_taggedartists_df = pd.read_csv(\n",
    "    user_taggedartists_file,\n",
    "    sep=\"\\t\",\n",
    "    encoding=\"ascii\"\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Display the shape after the data is loaded\n",
    "# ---------------------------\n",
    "print(\"Data loading completed:\")\n",
    "print(f\"- artists_df (item features): {artists_df.shape}\")\n",
    "print(f\"- tags_df: {tags_df.shape}\")\n",
    "print(f\"- user_artists_df (user features): {user_artists_df.shape}\")\n",
    "print(f\"- user_friends_df: {user_friends_df.shape}\")\n",
    "print(f\"- user_taggedartists_ts_df (interaction data part): {user_taggedartists_ts_df.shape}\")\n",
    "print(f\"- user_taggedartists_df (interaction data part): {user_taggedartists_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# ---------------------------\n",
    "# Construct item_features_df\n",
    "# ---------------------------\n",
    "# Item features only use id and name from artists.dat\n",
    "item_features_df = artists_df.copy()\n",
    "item_features_df.rename(columns={\"id\": \"item_id\", \"name\": \"artist_name\"}, inplace=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Construct user_features_df\n",
    "# ---------------------------\n",
    "# User features use userID, artistID, weight from user_artists.dat, and add artist_name based on artistID\n",
    "# First, establish a mapping dictionary from artistID -> artist_name\n",
    "artist_name_map = artists_df.set_index(\"id\")[\"name\"].to_dict()\n",
    "\n",
    "# Extract unique userID from user_artists_df to build user_features_df\n",
    "user_features_df = pd.DataFrame({'user_id': user_artists_df['userID'].unique()})\n",
    "\n",
    "# ---------------------------\n",
    "# Aggregate user_friends.dat data\n",
    "# ---------------------------\n",
    "# Write all friendID corresponding to each userID in user_friends.dat together\n",
    "aggregated_friends = user_friends_df.groupby(\"userID\")[\"friendID\"].apply(list).reset_index()\n",
    "aggregated_friends.rename(columns={\"userID\": \"user_id\", \"friendID\": \"friendID\"}, inplace=True)\n",
    "\n",
    "# Merge the aggregated friend information into user_features_df (using user_id as the key)\n",
    "user_features_df = pd.merge(user_features_df, aggregated_friends, on=\"user_id\", how=\"left\")\n",
    "\n",
    "# ---------------------------\n",
    "# Construct interaction_df\n",
    "# ---------------------------\n",
    "# Merge user_taggedartists-timestamps.dat with user_taggedartists.dat\n",
    "interaction_df = pd.merge(\n",
    "    user_taggedartists_ts_df,\n",
    "    user_taggedartists_df[[\"userID\", \"artistID\", \"tagID\", \"day\", \"month\", \"year\"]],\n",
    "    on=[\"userID\", \"artistID\", \"tagID\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Use tags_df to build a tagID -> tagValue mapping\n",
    "tag_value_map = tags_df.set_index(\"tagID\")[\"tagValue\"].to_dict()\n",
    "interaction_df[\"tagValue\"] = interaction_df[\"tagID\"].map(lambda x: tag_value_map.get(x, \"\"))\n",
    "\n",
    "# Rename columns to conform to a unified format\n",
    "interaction_df.rename(columns={\"userID\": \"user_id\", \"artistID\": \"item_id\"}, inplace=True)\n",
    "\n",
    "item_features_df[\"tags\"] = interaction_df.groupby(\"item_id\")[\"tagValue\"].apply(list).apply(lambda x: Counter(x).most_common(5))\n",
    "\n",
    "# Aggregate data based on user_id and item_id\n",
    "interaction_df = interaction_df.groupby(['user_id', 'item_id']).agg({\n",
    "    'timestamp': 'first',  # Keep the first timestamp\n",
    "    'day': 'first',  # Aggregate different values into a list\n",
    "    'month': 'first',\n",
    "    'year': 'first',\n",
    "    'tagID': lambda x: list(set(x)),\n",
    "    'tagValue': lambda x: list(set(x))\n",
    "}).reset_index()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Display the shape and header data of the final dataframes\n",
    "# ---------------------------\n",
    "print(\"\\nShape of the final dataframes:\")\n",
    "print(f\"item_features_df: {item_features_df.shape}\")\n",
    "print(f\"user_features_df: {user_features_df.shape}\")\n",
    "print(f\"interaction_df: {interaction_df.shape}\")\n",
    "\n",
    "print(\"\\nitem_features_df header data:\")\n",
    "display(item_features_df.head())\n",
    "\n",
    "print(\"\\nuser_features_df header data:\")\n",
    "display(user_features_df.head())\n",
    "\n",
    "print(\"\\ninteraction_df header data:\")\n",
    "display(interaction_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Convert datasets to a unified format\n",
    "\n",
    "# Convert interaction data\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "def process_item_df(row):\n",
    "    \"\"\"\n",
    "    Process each row in the item features dataframe\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"item_id\": row[\"item_id\"],\n",
    "        \"item_description\": {\n",
    "            \"name\": row[\"artist_name\"],\n",
    "            \"tags\": \"/\".join([tag for tag, _ in row[\"tags\"]]) if isinstance(row[\"tags\"], list) else \"\"\n",
    "        },\n",
    "        \"item_features\": {\n",
    "            \"tags\": row[\"tags\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "def process_user_df(row):\n",
    "    \"\"\"\n",
    "    Process each row in the user features dataframe, leaving empty if information is missing in the dataset\n",
    "    \"\"\" \n",
    "    return {\n",
    "        \"user_id\": row[\"user_id\"],\n",
    "        \"user_description\": {\n",
    "        },\n",
    "        \"user_features\": {\n",
    "            \"friends\": row[\"friendID\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "def process_interaction_df(row):\n",
    "    \"\"\"\n",
    "    Process the interaction dataframe\n",
    "    \"\"\" \n",
    "    return {\n",
    "        \"user_id\": row[\"user_id\"],\n",
    "        \"item_id\": row[\"item_id\"],\n",
    "        \"timestamp\": row[\"timestamp\"],\n",
    "        \"behavior_features\": {\n",
    "            \"tagID\": row[\"tagID\"],\n",
    "            \"tagValue\": row[\"tagValue\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "# Apply the above functions to each row of the respective dataframes and convert to List\n",
    "item_df = item_features_df.progress_apply(process_item_df, axis=1).to_list()\n",
    "user_df = user_features_df.progress_apply(process_user_df, axis=1).to_list()\n",
    "interaction_df = interaction_df.progress_apply(process_interaction_df, axis=1).to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Save the data\n",
    "\n",
    "# Convert the processed data into pandas dataframes\n",
    "item_df = pd.DataFrame(item_df)\n",
    "user_df = pd.DataFrame(user_df)\n",
    "interaction_df = pd.DataFrame(interaction_df)\n",
    "\n",
    "print(f\"\\nFinal shape of the dataframes:\")\n",
    "print(f\"Items: {len(item_df)}\")\n",
    "print(f\"Users: {len(user_df)}\")\n",
    "print(f\"Interactions: {len(interaction_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_df['timestamp'] = pd.to_datetime(interaction_df['timestamp'], unit='ms').dt.strftime('%Y-%m-%d %H:%M:%S')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_df = interaction_df[\n",
    "    interaction_df['user_id'].isin(user_df['user_id']) & \n",
    "    interaction_df['item_id'].isin(item_df['item_id'])\n",
    "]\n",
    "interaction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed data\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "print(\"\\nSaving files...\")\n",
    "\n",
    "interaction_df.to_json(os.path.join(OUTPUT_PATH, f\"{DATASET_NAME}_interaction.jsonl\"), lines=True, orient=\"records\")\n",
    "user_df.to_json(os.path.join(OUTPUT_PATH, f\"{DATASET_NAME}_user_feature.jsonl\"), lines=True, orient=\"records\")\n",
    "item_df.to_json(os.path.join(OUTPUT_PATH, f\"{DATASET_NAME}_item_feature.jsonl\"), lines=True, orient=\"records\")\n",
    "\n",
    "print(\"\\nProcessing complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
