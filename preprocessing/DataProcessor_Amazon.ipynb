{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Construction - Amazon\n",
    "\n",
    "This notebook aims to model any recommender system datasets to the unified sequential format of pure text string, serving as the input of the user simulator LLM.\n",
    "\n",
    "A recommender system data usually consists of 3 critical components: user, item and behavior. The user and item are usually represented by their IDs, while the behavior is usually represented by their interactions.\n",
    "\n",
    "The goal of this notebook is to transform the original data into a unified format, with two types of JSON format:\n",
    "\n",
    "- Item Feature JSON: This JSON contains a list of all items in the dataset, each element is following the format:\n",
    "```json\n",
    "{\n",
    "    \"item_id\": \"item_id (str), a unique identifier for the item\",\n",
    "    \"item_description\": \"item_description (dict[str, str]), a dictionary of item attributes and their values. All values should be processed to be pure text string that can be understood by human. All features that contain unreadable information (e.g. image, video, audio, url, ID string ,etc.) should be converted (when possible) or removed (when not possible) to be pure text string.\",\n",
    "    \"item_features\": \"item_features (dict[str, Any]), a dictionary of item features (except for those in item_description) and their values. \"\n",
    "This JSON is stored as `<DATASET_NAME>_item_feature.jsonl`.\n",
    "\n",
    "- User Feature JSON: This JSON contains a list of all items in the dataset, each element is following the format:\n",
    "```json\n",
    "{\n",
    "    \"user_id\": \"user_id (str), a unique identifier for the user\",\n",
    "    \"user_description\": \"user_description (dict[str, str]), a dictionary of user attributes and their values. All values should be processed to be pure text string that can be understood by human. All features that contain unreadable information (e.g. image, video, audio, url, ID string ,etc.) should be converted (when possible) or removed (when not possible) to be pure text string.\",\n",
    "    \"user_features\": \"user_features (dict[str, Any]), a dictionary of user features (except for those in user_description) and their values. \"\n",
    "}\n",
    "```\n",
    "This JSON is stored as `<DATASET_NAME>_user_feature.jsonl`.\n",
    "\n",
    "- Interaction JSON: This JSON contains a list of all user behaviors in the dataset, each element is following the format:\n",
    "```json\n",
    "{\n",
    "    \"user_id\": \"user_id (str), a unique identifier for the user\",\n",
    "    \"item_id\": \"item_id (str), the ID of the item that the user has interacted with\",\n",
    "    \"timestamp\": \"timestamp (str), the timestamp of the interaction in the format of YYYY-MM-DD HH:MM:SS. When the timestamp is not available, it should be set to the random timestamp.\",\n",
    "    \"behavior_features\": \"behavior_features (dict[str, Any]), a dictionary of behavior features and their values. All values should be processed to be pure text string that can be understood by human. All features that contain unreadable information (e.g. image, video, audio, url, ID string ,etc.) should be converted (when possible) or removed (when not possible) to be pure text string.\"\n",
    "}\n",
    "```\n",
    "This JSON is stored as `<DATASET_NAME>_interaction.jsonl`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the raw data\n",
    "\n",
    "You can download the raw data from the following links:\n",
    "- [Amazon](https://amazon-reviews-2023.github.io/)\n",
    "\n",
    "You should download the following files:\n",
    "- `meta_<SUBCATEGORY>.jsonl.gz`\n",
    "- `<SUBCATEGORY>.jsonl.gz`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Setting of the Filtered Dataset\n",
    "DATASET_NAME = \"Amazon-Fashion\"\n",
    "SUBCATEGORY = \"Amazon_Fashion\"\n",
    "\n",
    "DATASET_PATH = \"<SOURCE_PATH>\"\n",
    "OUTPUT_PATH = \"<PROJECT_PATH>/raws/\"\n",
    "MIN_INTERACTION_CNT = 5     # The minimum number of interactions for a user to be included in the dataset.\n",
    "MAX_INTERACTION_CNT = 20  # The maximum number of interactions for a user to be included in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files and subfolders in dataset directory:\n",
      "Amazon/\n",
      "    Industrial_and_Scientific.jsonl.gz\n",
      "    Baby_Products.jsonl.gz\n",
      "    meta_All_Beauty.jsonl.gz\n",
      "    Musical_Instruments.jsonl.gz\n",
      "    All_Beauty.json.gz\n",
      "    Appliances.jsonl.gz\n",
      "    meta_Books.jsonl.gz\n",
      "    meta_Office_Products.json\n",
      "    reviews_Office_Products.json\n",
      "    meta_Beauty_and_Personal_Care.jsonl.gz\n",
      "    meta_Health_and_Personal_Care.jsonl.gz\n",
      "    Video_Games.jsonl.gz\n",
      "    reviews_Video_Games.json\n",
      "    meta_Digital_Music.jsonl.gz\n",
      "    Beauty_and_Personal_Care.jsonl.gz\n",
      "    Software.jsonl.gz\n",
      "    meta_Musical_Instruments.jsonl.gz\n",
      "    meta_Baby_Products.jsonl.gz\n",
      "    meta_Amazon_Fashion.jsonl.gz\n",
      "    All_Beauty.jsonl.gz\n",
      "    Office_Products.jsonl.gz\n",
      "    CDs_and_Vinyl.jsonl.gz\n",
      "    Books.jsonl.gz\n",
      "    meta_Musical_Instruments.json\n",
      "    meta_Grocery_and_Gourmet_Food.jsonl.gz\n",
      "    reviews_Musical_Instruments.json\n",
      "    meta_Industrial_and_Scientific.jsonl.gz\n",
      "    Health_and_Personal_Care.jsonl.gz\n",
      "    meta_All_Beauty.json.gz\n",
      "    Grocery_and_Gourmet_Food.jsonl.gz\n",
      "    meta_CDs_and_Vinyl.jsonl.gz\n",
      "    meta_Software.jsonl.gz\n",
      "    meta_Video_Games.json\n",
      "    Amazon_Fashion.jsonl.gz\n",
      "    Digital_Music.jsonl.gz\n",
      "    meta_Appliances.jsonl.gz\n",
      "    meta_Video_Games.jsonl.gz\n",
      "    meta_Office_Products.jsonl.gz\n",
      "    amazon18/\n",
      "        meta_Appliances.json\n",
      "        Musical_Instruments.json\n",
      "        meta_Industrial_and_Scientific.json\n",
      "        meta_AMAZON_FASHION.json\n",
      "        meta_All_Beauty.json\n",
      "        All_Beauty.json\n",
      "        meta_Musical_Instruments.json\n",
      "        AMAZON_FASHION.json\n",
      "        Industrial_and_Scientific.json\n",
      "        Appliances.json\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Identify the file structure of the dataset\n",
    "\n",
    "import os\n",
    "# List all files and subfolders in the dataset directory, excluding hidden ones\n",
    "print(\"Files and subfolders in dataset directory:\")\n",
    "for root, dirs, files in os.walk(DATASET_PATH):\n",
    "    # Remove hidden directories\n",
    "    dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
    "    \n",
    "    level = root.replace(DATASET_PATH, '').count(os.sep)\n",
    "    indent = ' ' * 4 * level\n",
    "    folder_name = os.path.basename(root)\n",
    "    if not folder_name.startswith('.'):\n",
    "        print(f\"{indent}{folder_name}/\")\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        # Only show non-hidden files\n",
    "        visible_files = [f for f in files if not f.startswith('.')]\n",
    "        for f in visible_files:\n",
    "            print(f\"{subindent}{f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**File Structure**\n",
    "> Fill the file structure here so the LLM assistant can write the code to load the dataset.\n",
    "\n",
    "```txt\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Readme Content**\n",
    "> Fill the readme content here so the LLM assistant can write the code to load the dataset.\n",
    "\n",
    "```txt\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item features and daily features loaded\n",
      "Interactions loaded\n",
      "User features loaded\n",
      "Loaded dataframes shapes:\n",
      "- Interactions: (2500939, 10)\n",
      "- User features: (2035490, 1)\n",
      "- Item features: (826108, 14)\n",
      "Data loaded\n",
      "Interaction dataframe head:\n",
      "   rating                 title  \\\n",
      "0       5         Pretty locket   \n",
      "1       5                     A   \n",
      "2       2             Two Stars   \n",
      "3       1       Wonâ€™t buy again   \n",
      "4       5  I LOVE these glasses   \n",
      "\n",
      "                                                text images        asin  \\\n",
      "0  I think this locket is really pretty. The insi...     []  B00LOPVX74   \n",
      "1                                              Great     []  B07B4JXK8D   \n",
      "2  One of the stones fell out within the first 2 ...     []  B007ZSEQ4Q   \n",
      "3  Crappy socks. Money wasted. Bought to wear wit...     []  B07F2BTFS9   \n",
      "4  I LOVE these glasses!  They fit perfectly over...     []  B00PKRFU4O   \n",
      "\n",
      "  parent_asin                       user_id               timestamp  \\\n",
      "0  B00LOPVX74  AGBFYI2DDIKXC5Y4FARTYDTQBMFQ 2020-01-09 00:06:34.489   \n",
      "1  B07B4JXK8D  AFQLNQNQYFWQZPJQZS6V3NZU4QBQ 2020-12-20 01:04:06.701   \n",
      "2  B007ZSEQ4Q  AHITBJSS7KYUBVZPX7M2WJCOIVKQ 2015-05-23 01:33:48.000   \n",
      "3  B07F2BTFS9  AFVNEEPDEIH5SPUN5BWC6NKL3WNQ 2018-12-31 20:57:27.095   \n",
      "4  B00XESJTDE  AHSPLDNW5OOUK2PLH7GXLACFBZNQ 2015-08-13 14:29:26.000   \n",
      "\n",
      "   helpful_vote  verified_purchase  \n",
      "0             3               True  \n",
      "1             0               True  \n",
      "2             3               True  \n",
      "3             2               True  \n",
      "4             0               True  \n",
      "User features dataframe head:\n",
      "                        user_id\n",
      "0  AGBFYI2DDIKXC5Y4FARTYDTQBMFQ\n",
      "1  AFQLNQNQYFWQZPJQZS6V3NZU4QBQ\n",
      "2  AHITBJSS7KYUBVZPX7M2WJCOIVKQ\n",
      "3  AFVNEEPDEIH5SPUN5BWC6NKL3WNQ\n",
      "4  AHSPLDNW5OOUK2PLH7GXLACFBZNQ\n",
      "Item features dataframe head:\n",
      "    main_category                                              title  \\\n",
      "0  AMAZON FASHION  YUEDGE 5 Pairs Men's Moisture Control Cushione...   \n",
      "1  AMAZON FASHION  DouBCQ Women's Palazzo Lounge Wide Leg Casual ...   \n",
      "2  AMAZON FASHION  Pastel by Vivienne Honey Vanilla Girls' Trapez...   \n",
      "3  AMAZON FASHION                                   Mento Streamtail   \n",
      "4  AMAZON FASHION  RONNOX Women's 3-Pairs Bright Colored Calf Com...   \n",
      "\n",
      "   average_rating  rating_number  \\\n",
      "0             4.6             16   \n",
      "1             4.1              7   \n",
      "2             4.3             11   \n",
      "3             2.0              1   \n",
      "4             4.3           3032   \n",
      "\n",
      "                                            features  \\\n",
      "0                                                 []   \n",
      "1                 [Drawstring closure, Machine Wash]   \n",
      "2                   [Zipper closure, Hand Wash Only]   \n",
      "3  [Thermoplastic Rubber sole, High Density Premi...   \n",
      "4  [Pull On closure, Size Guide: \"S\" fits calf 10...   \n",
      "\n",
      "                                         description  price  \\\n",
      "0                                                 []    NaN   \n",
      "1                                                 []    NaN   \n",
      "2                                                 []    NaN   \n",
      "3  [Slip on the Women's Mento and you're ready to...  29.81   \n",
      "4  [Ronnox Calf Sleeves - Allowing Your Body to P...  17.99   \n",
      "\n",
      "                                              images  \\\n",
      "0  [{'thumb': 'https://m.media-amazon.com/images/...   \n",
      "1  [{'thumb': 'https://m.media-amazon.com/images/...   \n",
      "2  [{'thumb': 'https://m.media-amazon.com/images/...   \n",
      "3  [{'thumb': 'https://m.media-amazon.com/images/...   \n",
      "4  [{'thumb': 'https://m.media-amazon.com/images/...   \n",
      "\n",
      "                                              videos               store  \\\n",
      "0                                                 []            GiveGift   \n",
      "1                                                 []              DouBCQ   \n",
      "2                                                 []  Pastel by Vivienne   \n",
      "3                                                 []          Guy Harvey   \n",
      "4  [{'title': 'HONEST Review: RONNOX Women's 3-Pa...              RONNOX   \n",
      "\n",
      "  categories                                            details parent_asin  \\\n",
      "0         []  {'Package Dimensions': '10.31 x 8.5 x 1.73 inc...  B08BHN9PK5   \n",
      "1         []  {'Package Dimensions': '15 x 10.2 x 0.4 inches...  B08R39MRDW   \n",
      "2         []  {'Is Discontinued By Manufacturer': 'No', 'Pac...  B077KJHCJ4   \n",
      "3         []  {'Package Dimensions': '11.22 x 4.72 x 4.33 in...  B0811M2JG9   \n",
      "4         []  {'Is Discontinued By Manufacturer': 'No', 'Pac...  B07SB2892S   \n",
      "\n",
      "   bought_together  \n",
      "0              NaN  \n",
      "1              NaN  \n",
      "2              NaN  \n",
      "3              NaN  \n",
      "4              NaN  \n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load the dataset\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the item features\n",
    "item_features_df = pd.read_json(DATASET_PATH + f'/meta_{SUBCATEGORY}.jsonl.gz', lines=True, compression='gzip')\n",
    "item_features_df = item_features_df.drop_duplicates(subset='parent_asin')\n",
    "print(\"Item features and daily features loaded\")\n",
    "\n",
    "# Load the interactions\n",
    "interaction_df = pd.read_json(DATASET_PATH + f'/{SUBCATEGORY}.jsonl.gz', lines=True, compression='gzip')\n",
    "print(\"Interactions loaded\")\n",
    "\n",
    "# Generate user features\n",
    "user_df = interaction_df[['user_id']].drop_duplicates(subset='user_id')\n",
    "print(\"User features loaded\")\n",
    "\n",
    "# Display column names and sample values for each dataframe\n",
    "print(\"Loaded dataframes shapes:\")\n",
    "print(f\"- Interactions: {interaction_df.shape}\")\n",
    "print(f\"- User features: {user_df.shape}\")\n",
    "print(f\"- Item features: {item_features_df.shape}\")\n",
    "\n",
    "print(\"Data loaded\")\n",
    "# print head\n",
    "print(\"Interaction dataframe head:\")\n",
    "print(interaction_df.head())\n",
    "print(\"User features dataframe head:\")\n",
    "print(user_df.head())\n",
    "print(\"Item features dataframe head:\")\n",
    "print(item_features_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Descriptions**\n",
    "> Fill the dataframe descriptions loaded from the dataset here so the LLM assistant can write the code to load the dataset.\n",
    "\n",
    "```txt\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Filter the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beauty, Fashion\n",
    "MIN_INTERACTION_CNT = 5\n",
    "MAX_INTERACTION_CNT = 20\n",
    "MIN_RATING_NUMBER = 10\n",
    "MIN_DETAILS_LENGTH = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampling\n",
    "\n",
    "interaction_df = interaction_df.sort_values(by=\"timestamp\").reset_index(drop=True)\n",
    "interaction_df = interaction_df[int(len(interaction_df) * 0.7):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Interactions: (5025, 10)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Filter the dataset\n",
    "# Filter the dataset with the minimum and maximum number of interactions\n",
    "\n",
    "\n",
    "user_count = interaction_df.groupby('user_id').size()\n",
    "user_count = user_count[user_count >= MIN_INTERACTION_CNT]\n",
    "\n",
    "item_count = interaction_df.groupby('parent_asin').size()\n",
    "item_count = item_count[item_count >= MIN_INTERACTION_CNT]\n",
    "\n",
    "item_features_df = item_features_df[item_features_df['rating_number'] >= MIN_RATING_NUMBER]\n",
    "item_features_df = item_features_df[item_features_df[\"details\"].apply(len) > MIN_DETAILS_LENGTH]\n",
    "item_features_df = item_features_df[item_features_df['parent_asin'].isin(item_count.index)]\n",
    "interaction_df = interaction_df[interaction_df['parent_asin'].isin(item_features_df['parent_asin'])]\n",
    "\n",
    "interaction_df = interaction_df[interaction_df['user_id'].isin(user_count.index)]\n",
    "interaction_df = interaction_df[interaction_df['parent_asin'].isin(item_features_df['parent_asin']) & interaction_df['user_id'].isin(user_df['user_id'])]\n",
    "\n",
    "print(f\"- Interactions: {interaction_df.shape}\")\n",
    "# If there is constraints in settings, apply the constraints to the dataset to filter the dataset.\n",
    "# Otherwise, skip this step.\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18578, 14)\n"
     ]
    }
   ],
   "source": [
    "inter_asin = interaction_df[\"parent_asin\"].unique()\n",
    "inter_interaction_df = item_features_df[item_features_df['parent_asin'].isin(inter_asin)]\n",
    "\n",
    "outer_interaction_df = item_features_df[~item_features_df['parent_asin'].isin(inter_asin)]\n",
    "\n",
    "if len(outer_interaction_df) > 65536:\n",
    "    outer_interaction_df = outer_interaction_df.sample(65536, random_state=0)\n",
    "\n",
    "concat_item_features_df = pd.concat([\n",
    "    inter_interaction_df,\n",
    "    outer_interaction_df\n",
    "]).drop_duplicates(subset='parent_asin')\n",
    "\n",
    "item_features_df = concat_item_features_df\n",
    "\n",
    "print(item_features_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Convert the dataset to the unified format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18578/18578 [00:00<00:00, 51257.45it/s]\n",
      "Processing users: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1797/1797 [00:00<00:00, 213880.94it/s]\n",
      "Processing interactions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5025/5025 [00:00<00:00, 71163.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Convert the dataset to the unified format\n",
    "\n",
    "# Use tqdm and pandas.progress_apply to process the dataset.\n",
    "# Based on the data descriptions, convert the dataset to 3 dataframes: item_df, user_df, interaction_df.\n",
    "# Write 3 row processing functions: process_item_df, process_user_df, process_interaction_df.\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "def convert_list_to_str(description_list):\n",
    "    # Convert the list to a string with each element separated by a space\n",
    "    if len(description_list) == 0:\n",
    "        return \"\"\n",
    "    return ', '.join(description_list).strip() + \"\\n\"\n",
    "\n",
    "def convert_dict_to_str(description_dict):\n",
    "    if len(description_dict) == 0:\n",
    "        return \"\"\n",
    "    return ';'.join([f\"{k}: {v}\" for k, v in description_dict.items()]).strip() + \"\\n\"\n",
    "\n",
    "def process_item_df(row):\n",
    "    return {\n",
    "        \"item_id\": row['parent_asin'],\n",
    "        \"item_description\": {\n",
    "            \"title\": row['title'],\n",
    "            \"price\": row['price'] if row['price'] else \"NA\",\n",
    "            \"store\": row['store'],\n",
    "            \"average_rating\": row['average_rating'],\n",
    "            \"rating_number\": row['rating_number'],\n",
    "            \"category\": \"-\".join(row['categories']) if len(row['categories']) > 0 else str(row['main_category']),\n",
    "            \"description\": convert_dict_to_str(row['details']),\n",
    "            \"features\": convert_list_to_str(row['features'])\n",
    "        },\n",
    "        \"item_features\": {\n",
    "\n",
    "        }\n",
    "    }\n",
    "\n",
    "def process_user_df(row):\n",
    "    return {\n",
    "        \"user_id\": row['user_id'],\n",
    "        \"user_description\": {\n",
    "        },\n",
    "        \"user_features\": {\n",
    "        }\n",
    "    }\n",
    "\n",
    "def process_interaction_df(row):\n",
    "    return {\n",
    "        \"user_id\": row['user_id'],\n",
    "        \"item_id\": row['parent_asin'],\n",
    "        \"timestamp\": datetime.strftime(row['timestamp'], '%Y-%m-%d %H:%M:%S.%f'),\n",
    "        \"behavior_features\": {\n",
    "            \"rating\": row['rating'],\n",
    "            \"review_text\": row['text'],\n",
    "            \"review_summary\": row['title']\n",
    "        }\n",
    "    }\n",
    "tqdm.pandas(desc=\"Processing items\")\n",
    "item_records = item_features_df.progress_apply(process_item_df, axis=1).tolist()\n",
    "\n",
    "# Process users \n",
    "tqdm.pandas(desc=\"Processing users\")\n",
    "user_df = user_df[user_df['user_id'].isin(interaction_df['user_id'].unique())]\n",
    "user_records = user_df.progress_apply(process_user_df, axis=1).tolist()\n",
    "\n",
    "# Process interactions\n",
    "tqdm.pandas(desc=\"Processing interactions\")\n",
    "interaction_records = interaction_df.progress_apply(process_interaction_df, axis=1).dropna().tolist()\n",
    "\n",
    "# Apply the processing functions to the dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interaction dataframe head:\n",
      "                        user_id     item_id                   timestamp  \\\n",
      "0  AHX325QNRR25RGJG44MKHYCMTA7Q  B083CT6ZZ2  2020-03-04 15:40:08.116000   \n",
      "1  AEPEDW5FBBJ2XYR2BIJAKUPHCMHA  B083CZSGM3  2020-03-04 20:08:27.604000   \n",
      "2  AH3BXW7KLIS2VAE56UXJS2NS7I5A  B083CT6ZZ2  2020-03-04 22:35:23.110000   \n",
      "3  AHV6QCNBJNSGLATP56JAWJ3C4G2A  B07W8FK4CX  2020-03-05 03:53:51.592000   \n",
      "4  AGK7OPKYWYPCFR52AZHDCBWPVCPQ  B083CT6ZZ2  2020-03-05 14:02:33.331000   \n",
      "\n",
      "                                   behavior_features  \n",
      "0  {'rating': 3, 'review_text': 'This is a really...  \n",
      "1  {'rating': 5, 'review_text': 'This is a very n...  \n",
      "2  {'rating': 4, 'review_text': 'I didn't have hi...  \n",
      "3  {'rating': 4, 'review_text': 'My daughter part...  \n",
      "4  {'rating': 3, 'review_text': 'First, let me st...  \n",
      "User features dataframe head:\n",
      "                        user_id user_description user_features\n",
      "0  AFZUK3MTBIBEDQOPAK3OATUOUKLA               {}            {}\n",
      "1  AFFZVSTUS3U2ZD22A2NPZSKOCPGQ               {}            {}\n",
      "2  AHV6QCNBJNSGLATP56JAWJ3C4G2A               {}            {}\n",
      "3  AFJBKPK5W56XWSNPQU2WW66ISWYQ               {}            {}\n",
      "4  AFDMZ4TRX3HXQQUGWAHJQTIF65BQ               {}            {}\n",
      "Item features dataframe head:\n",
      "      item_id                                   item_description item_features\n",
      "0  B09167DC63  {'title': 'YANPENG Cute French Bulldog Key Pen...            {}\n",
      "1  B083K4F2H7  {'title': 'H2H Mens Active Short Sleeve Polo L...            {}\n",
      "2  B07S8KMCQW  {'title': 'icyzone Workout Tank Tops for Women...            {}\n",
      "3  B09LLZZFLB  {'title': 'ALPHA CAMP Men's Long-Sleeve Hooded...            {}\n",
      "4  B08PFYDC1J  {'title': 'CRZ YOGA Women's Long Sleeves Worko...            {}\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Save the dataset\n",
    "import os\n",
    "# Convert records to pandas DataFrames\n",
    "interaction_df = pd.DataFrame.from_records(interaction_records)\n",
    "user_df = pd.DataFrame.from_records(user_records) \n",
    "item_df = pd.DataFrame.from_records(item_records)\n",
    "\n",
    "print(\"Interaction dataframe head:\")\n",
    "print(interaction_df.head())\n",
    "print(\"User features dataframe head:\")\n",
    "print(user_df.head())\n",
    "print(\"Item features dataframe head:\")\n",
    "print(item_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "interaction_df.to_json(os.path.join(OUTPUT_PATH, f\"{DATASET_NAME}_interaction.jsonl\"), lines=True, orient=\"records\")\n",
    "user_df.to_json(os.path.join(OUTPUT_PATH, f\"{DATASET_NAME}_user_feature.jsonl\"), lines=True, orient=\"records\")\n",
    "item_df.to_json(os.path.join(OUTPUT_PATH, f\"{DATASET_NAME}_item_feature.jsonl\"), lines=True, orient=\"records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
